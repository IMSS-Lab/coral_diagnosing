{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Duke Coral Acidification Model (PyTorch Implementation)\n",
    "\n",
    "This notebook contains comprehensive improvements to the coral reef acidification prediction model, including:\n",
    "\n",
    "1. Enhanced model architecture with attention mechanisms and pre-trained backbones\n",
    "2. Improved training processes with advanced techniques\n",
    "3. Robust evaluation and validation approaches\n",
    "4. Uncertainty quantification methods\n",
    "5. Advanced explainability techniques\n",
    "6. Optimized data processing and feature selection\n",
    "7. Performance optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "import pywt\n",
    "import cv2\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split, WeightedRandomSampler\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torchmetrics\n",
    "torch.set_float32_matmul_precision('high')\n",
    "# Sklearn imports\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Explainability imports\n",
    "import shap\n",
    "from captum.attr import (\n",
    "    IntegratedGradients, GradientShap, DeepLift, GuidedGradCam,\n",
    "    NoiseTunnel, FeatureAblation\n",
    ")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing and Synthetic Data Generation\n",
    "\n",
    "Enhanced data processing pipeline with time-frequency analysis and better synthetic data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dimensions for synthetic data\n",
    "num_samples = 1000\n",
    "img_size = (224, 224, 3)  # Increased resolution for better feature extraction\n",
    "time_steps = 24  # Increased from 12 to capture more temporal patterns\n",
    "num_features = 8  # Increased from 5 to include more environmental variables\n",
    "\n",
    "# Define named features for better interpretability\n",
    "feature_names = [\n",
    "    'sea_surface_temperature',\n",
    "    'ph_level',\n",
    "    'dissolved_oxygen',\n",
    "    'salinity',\n",
    "    'turbidity',\n",
    "    'chlorophyll',\n",
    "    'aragonite_saturation',\n",
    "    'nitrate_concentration'\n",
    "]\n",
    "\n",
    "# Generate realistic synthetic data instead of completely random data\n",
    "def generate_synthetic_data(num_samples, img_size, time_steps, num_features):\n",
    "    # Create temporal patterns with realistic seasonal variations\n",
    "    time_indices = np.linspace(0, 2*np.pi, time_steps)\n",
    "    \n",
    "    # Initialize time series data\n",
    "    X_time_series = np.zeros((num_samples, time_steps, num_features))\n",
    "    \n",
    "    # Generate image data with coral-like patterns\n",
    "    X_images = np.zeros((num_samples, *img_size))\n",
    "    \n",
    "    # Generate labels with controlled class balance (60% healthy, 40% bleached)\n",
    "    y = np.zeros(num_samples)\n",
    "    \n",
    "    # Generate data for each sample\n",
    "    for i in range(num_samples):\n",
    "        # Determine if sample is a bleached coral (1) or healthy coral (0)\n",
    "        is_bleached = np.random.random() < 0.4  # 40% bleached samples\n",
    "        y[i] = 1 if is_bleached else 0\n",
    "        \n",
    "        # Generate time series data\n",
    "        for f in range(num_features):\n",
    "            # Base seasonal pattern\n",
    "            seasonal = 0.5 * np.sin(time_indices + f*0.5)\n",
    "            \n",
    "            # Add trend\n",
    "            trend = np.linspace(0, 0.3, time_steps) if is_bleached else np.linspace(0, 0.1, time_steps)\n",
    "            \n",
    "            # Add noise\n",
    "            noise = np.random.normal(0, 0.1, time_steps)\n",
    "            \n",
    "            # Feature-specific adjustments\n",
    "            if f == 0:  # Temperature: higher for bleached corals\n",
    "                base = 29 if is_bleached else 26\n",
    "                amplitude = 3 if is_bleached else 2\n",
    "                X_time_series[i, :, f] = base + amplitude * seasonal + trend + noise\n",
    "            \n",
    "            elif f == 1:  # pH: lower for bleached corals (more acidic)\n",
    "                base = 7.9 if is_bleached else 8.2\n",
    "                amplitude = 0.1\n",
    "                X_time_series[i, :, f] = base + amplitude * seasonal - trend + noise * 0.05\n",
    "            \n",
    "            elif f == 2:  # Dissolved oxygen: lower for bleached corals\n",
    "                base = 5 if is_bleached else 6.5\n",
    "                X_time_series[i, :, f] = base + seasonal - trend + noise\n",
    "            \n",
    "            else:  # Other features\n",
    "                base = np.random.uniform(0, 5)  # Random base value\n",
    "                X_time_series[i, :, f] = base + seasonal + (trend if is_bleached else -trend) + noise\n",
    "        \n",
    "        # Generate image data with coral-like textures\n",
    "        # For simplicity, we'll create patterns that look different for healthy vs bleached\n",
    "        img = np.zeros(img_size)\n",
    "        \n",
    "        # Create base texture\n",
    "        for c in range(3):  # RGB channels\n",
    "            noise = np.random.normal(0, 1, (img_size[0]//8, img_size[1]//8))\n",
    "            # Upscale noise to full image size\n",
    "            noise = cv2.resize(noise, (img_size[1], img_size[0]))\n",
    "            \n",
    "            # Add perlin-like noise for more natural textures\n",
    "            for scale in [4, 8, 16, 32]:\n",
    "                octave = np.random.normal(0, 1, (img_size[0]//scale, img_size[1]//scale))\n",
    "                octave = cv2.resize(octave, (img_size[1], img_size[0]))\n",
    "                noise += octave * (scale/32)\n",
    "            \n",
    "            img[:, :, c] = noise\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        img = (img - img.min()) / (img.max() - img.min())\n",
    "        \n",
    "        # Apply different color schemes for healthy vs bleached\n",
    "        if is_bleached:\n",
    "            # Bleached: whiter, less color variation\n",
    "            img = 0.8 + img * 0.2  # Higher base value for whiter appearance\n",
    "            # Reduce color saturation\n",
    "            img[:, :, 1] = img[:, :, 1] * 0.3 + img[:, :, 0] * 0.7  # Less green\n",
    "            img[:, :, 2] = img[:, :, 2] * 0.3 + img[:, :, 0] * 0.7  # Less blue\n",
    "        else:\n",
    "            # Healthy: more colorful, especially in reds and browns\n",
    "            img[:, :, 0] = img[:, :, 0] * 0.9  # More red\n",
    "            img[:, :, 1] = img[:, :, 1] * 0.7  # Less green\n",
    "            img[:, :, 2] = img[:, :, 2] * 0.6  # Less blue\n",
    "        \n",
    "        X_images[i] = img\n",
    "    \n",
    "    return X_images, X_time_series, y\n",
    "\n",
    "# Generate the synthetic dataset\n",
    "X_images, X_time_series, y = generate_synthetic_data(num_samples, img_size, time_steps, num_features)\n",
    "\n",
    "# Display dataset statistics\n",
    "print(f\"Dataset size: {num_samples} samples\")\n",
    "print(f\"Class distribution: {np.sum(y == 0)} healthy, {np.sum(y == 1)} bleached\")\n",
    "print(f\"Image shape: {X_images.shape}\")\n",
    "print(f\"Time series shape: {X_time_series.shape}\")\n",
    "print(f\"Image shape before permute: {X_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply time-frequency analysis to time series data\n",
    "def extract_time_frequency_features(X_time_series):\n",
    "    num_samples, time_steps, num_features = X_time_series.shape\n",
    "    \n",
    "    # We'll extract wavelet coefficients using PyWavelets\n",
    "    wavelet = 'db4'  # Daubechies wavelet\n",
    "    level = 3  # Decomposition level\n",
    "    \n",
    "    # Container for wavelet features\n",
    "    wavelet_features = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        sample_features = []\n",
    "        \n",
    "        for f in range(num_features):\n",
    "            # Get time series for this feature\n",
    "            ts = X_time_series[i, :, f]\n",
    "            \n",
    "            # Perform wavelet decomposition\n",
    "            coeffs = pywt.wavedec(ts, wavelet, level=level)\n",
    "            \n",
    "            # Extract statistical features from each coefficient level\n",
    "            features = []\n",
    "            for coeff in coeffs:\n",
    "                features.extend([np.mean(coeff), np.std(coeff), np.max(coeff), np.min(coeff)])\n",
    "            \n",
    "            sample_features.extend(features)\n",
    "        \n",
    "        wavelet_features.append(sample_features)\n",
    "    \n",
    "    return np.array(wavelet_features)\n",
    "\n",
    "# Extract time-frequency features\n",
    "X_wavelet = extract_time_frequency_features(X_time_series)\n",
    "print(f\"Extracted wavelet features shape: {X_wavelet.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample data\n",
    "def plot_sample_data(X_images, X_time_series, y, sample_idx):\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Plot the image\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.imshow(X_images[sample_idx])\n",
    "    ax1.set_title(f\"Sample {sample_idx}: {'Bleached' if y[sample_idx] == 1 else 'Healthy'} Coral\")\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Plot the time series\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    for f in range(num_features):\n",
    "        ax2.plot(X_time_series[sample_idx, :, f], label=feature_names[f])\n",
    "    \n",
    "    ax2.set_title(f\"Time Series Data for Sample {sample_idx}\")\n",
    "    ax2.set_xlabel(\"Time Steps\")\n",
    "    ax2.set_ylabel(\"Value\")\n",
    "    ax2.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot a random healthy and bleached sample\n",
    "healthy_idx = np.where(y == 0)[0][0]\n",
    "bleached_idx = np.where(y == 1)[0][0]\n",
    "\n",
    "plot_sample_data(X_images, X_time_series, y, healthy_idx)\n",
    "plot_sample_data(X_images, X_time_series, y, bleached_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Dataset with multiple input sources\n",
    "class CoralDataset(Dataset):\n",
    "    def __init__(self, images, time_series, wavelet_features, labels, transform=None):\n",
    "        # Ensure images are in the correct format (N, H, W, C) -> (N, C, H, W)\n",
    "        self.images = torch.FloatTensor(images).permute(0, 3, 1, 2)\n",
    "        self.time_series = torch.FloatTensor(time_series)\n",
    "        self.wavelet_features = torch.FloatTensor(wavelet_features)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        time_series = self.time_series[idx]\n",
    "        wavelet = self.wavelet_features[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, time_series, wavelet, label\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Split data using stratified sampling\n",
    "X_train_img, X_test_img, X_train_ts, X_test_ts, X_train_wav, X_test_wav, y_train, y_test = train_test_split(\n",
    "    X_images, X_time_series, X_wavelet, y, \n",
    "    test_size=0.2, \n",
    "    random_state=SEED,\n",
    "    stratify=y  # Ensure class balance in train and test\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CoralDataset(X_train_img, X_train_ts, X_train_wav, y_train, transform)\n",
    "test_dataset = CoralDataset(X_test_img, X_test_ts, X_test_wav, y_test, transform)\n",
    "\n",
    "# Verify class distribution\n",
    "print(f\"Training set: {np.sum(y_train == 0)} healthy, {np.sum(y_train == 1)} bleached\")\n",
    "print(f\"Test set: {np.sum(y_test == 0)} healthy, {np.sum(y_test == 1)} bleached\")\n",
    "# Verify tensor shapes\n",
    "print(f\"Train dataset image shape: {train_dataset.images.shape}\")\n",
    "print(f\"Test dataset image shape: {test_dataset.images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted sampler to handle class imbalance\n",
    "def create_weighted_sampler(labels):\n",
    "    class_counts = np.bincount(labels.astype(int))\n",
    "    class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
    "    weights = class_weights[labels.astype(int)]\n",
    "    sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)\n",
    "    return sampler\n",
    "\n",
    "# Create sampler for training data\n",
    "train_sampler = create_weighted_sampler(y_train)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    sampler=train_sampler,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Enhanced Model Architecture with Attention Mechanisms and Pre-trained Backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-attention module for time series\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dim])).to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, hidden_dim]\n",
    "        batch_size, seq_len, hidden_dim = x.shape\n",
    "        \n",
    "        Q = self.query(x)  # [batch_size, seq_len, hidden_dim]\n",
    "        K = self.key(x)    # [batch_size, seq_len, hidden_dim]\n",
    "        V = self.value(x)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        energy = torch.matmul(Q, K.permute(0, 2, 1)) / self.scale  # [batch_size, seq_len, seq_len]\n",
    "        attention = F.softmax(energy, dim=-1)  # [batch_size, seq_len, seq_len]\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = torch.matmul(attention, V)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        return out, attention\n",
    "\n",
    "# CNN module with pre-trained backbone\n",
    "class CNNModule(nn.Module):\n",
    "    def __init__(self, pretrained=True, backbone='efficientnet'):\n",
    "        super(CNNModule, self).__init__()\n",
    "        \n",
    "        if backbone == 'efficientnet':\n",
    "            # Load pretrained EfficientNet\n",
    "            self.backbone = models.efficientnet_b0(pretrained=pretrained)\n",
    "            # Replace the classifier to get the features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            self.feature_dim = 1280\n",
    "        elif backbone == 'resnet':\n",
    "            # Load pretrained ResNet\n",
    "            self.backbone = models.resnet50(pretrained=pretrained)\n",
    "            # Replace the fully connected layer to get the features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "            self.feature_dim = 2048\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported backbone: {backbone}\")\n",
    "        \n",
    "        # Additional layers for coral-specific features\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(self.feature_dim, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, channels, height, width]\n",
    "        features = self.backbone(x)  # [batch_size, feature_dim]\n",
    "        features = self.dropout(features)\n",
    "        features = self.fc(features)\n",
    "        features = self.relu(features)\n",
    "        return features\n",
    "\n",
    "# LSTM module with attention for time series\n",
    "class LSTMModule(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2, bidirectional=True, dropout=0.3):\n",
    "        super(LSTMModule, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Self-attention layer\n",
    "        self.attention = SelfAttention(hidden_dim * self.num_directions)\n",
    "        \n",
    "        # Additional layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * self.num_directions, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, input_dim]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_dim).to(device)\n",
    "        \n",
    "        # Forward pass through LSTM\n",
    "        output, (hidden, cell) = self.lstm(x, (h0, c0))  # output: [batch_size, seq_len, hidden_dim * num_directions]\n",
    "        \n",
    "        # Apply self-attention\n",
    "        output, attention_weights = self.attention(output)\n",
    "        \n",
    "        # Global max pooling\n",
    "        output, _ = torch.max(output, dim=1)  # [batch_size, hidden_dim * num_directions]\n",
    "        \n",
    "        # Apply dropout and fully connected layer\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        output = self.relu(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Wavelet Feature Processing Module\n",
    "class WaveletModule(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, dropout=0.3):\n",
    "        super(WaveletModule, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim * 2)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, input_dim]\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# Combined model architecture\n",
    "class CoralNet(nn.Module):\n",
    "    def __init__(self, time_steps, num_features, wavelet_dim):\n",
    "        super(CoralNet, self).__init__()\n",
    "        \n",
    "        # CNN for image data\n",
    "        self.cnn_module = CNNModule(pretrained=True, backbone='efficientnet')\n",
    "        \n",
    "        # LSTM for time series data\n",
    "        self.lstm_module = LSTMModule(input_dim=num_features, hidden_dim=64, num_layers=2, bidirectional=True)\n",
    "        \n",
    "        # Wavelet feature processing\n",
    "        self.wavelet_module = WaveletModule(input_dim=wavelet_dim, hidden_dim=64)\n",
    "        \n",
    "        # Fusion layers\n",
    "        self.fusion_fc1 = nn.Linear(256 + 128 + 64, 128)\n",
    "        self.fusion_bn1 = nn.BatchNorm1d(128)\n",
    "        self.fusion_relu = nn.ReLU()\n",
    "        self.fusion_dropout = nn.Dropout(0.4)\n",
    "        self.fusion_fc2 = nn.Linear(128, 64)\n",
    "        self.fusion_bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_fc = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, image, time_series, wavelet):\n",
    "        # Process image data\n",
    "        image_features = self.cnn_module(image)  # [batch_size, 256]\n",
    "        \n",
    "        # Process time series data\n",
    "        time_features, attention_weights = self.lstm_module(time_series)  # [batch_size, 128], [batch_size, seq_len, seq_len]\n",
    "        \n",
    "        # Process wavelet features\n",
    "        wavelet_features = self.wavelet_module(wavelet)  # [batch_size, 64]\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined = torch.cat([image_features, time_features, wavelet_features], dim=1)  # [batch_size, 256+128+64]\n",
    "        \n",
    "        # Fusion layers\n",
    "        fusion = self.fusion_fc1(combined)\n",
    "        fusion = self.fusion_bn1(fusion)\n",
    "        fusion = self.fusion_relu(fusion)\n",
    "        fusion = self.fusion_dropout(fusion)\n",
    "        fusion = self.fusion_fc2(fusion)\n",
    "        fusion = self.fusion_bn2(fusion)\n",
    "        fusion = self.fusion_relu(fusion)\n",
    "        \n",
    "        # Output prediction\n",
    "        output = self.output_fc(fusion)        \n",
    "        return output.squeeze(), attention_weights\n",
    "\n",
    "# Initialize the model\n",
    "model = CoralNet(time_steps=time_steps, num_features=num_features, wavelet_dim=X_wavelet.shape[1]).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PyTorch Lightning Model with Advanced Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Lightning model for advanced training\n",
    "class CoralLightningModel(pl.LightningModule):\n",
    "    def __init__(self, time_steps, num_features, wavelet_dim, learning_rate=0.001):\n",
    "        super(CoralLightningModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = CoralNet(time_steps=time_steps, num_features=num_features, wavelet_dim=wavelet_dim)\n",
    "        \n",
    "        # Loss function with class weighting\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        # Metrics\n",
    "        self.train_acc = torchmetrics.Accuracy(task='binary')\n",
    "        self.val_acc = torchmetrics.Accuracy(task='binary')\n",
    "        self.val_auroc = torchmetrics.AUROC(task='binary')\n",
    "        self.val_f1 = torchmetrics.F1Score(task='binary')\n",
    "        self.val_precision = torchmetrics.Precision(task='binary')\n",
    "        self.val_recall = torchmetrics.Recall(task='binary')\n",
    "        \n",
    "        # For uncertainty estimation\n",
    "        self.mc_iterations = 10\n",
    "        self.dropout_active = False\n",
    "        \n",
    "    def forward(self, image, time_series, wavelet):\n",
    "        return self.model(image, time_series, wavelet)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.learning_rate, weight_decay=1e-4)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        image, time_series, wavelet, y = batch\n",
    "        y_hat, _ = self(image, time_series, wavelet)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "\n",
    "        # Log metrics with sigmoid\n",
    "        acc = self.train_acc(torch.sigmoid(y_hat), y.int())\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        image, time_series, wavelet, y = batch\n",
    "        y_hat, _ = self(image, time_series, wavelet)\n",
    "        val_loss = self.criterion(y_hat, y)\n",
    "        \n",
    "        # Log metrics with sigmoid\n",
    "        acc = self.val_acc(torch.sigmoid(y_hat), y.int())\n",
    "        auroc = self.val_auroc(torch.sigmoid(y_hat), y.int())\n",
    "        f1 = self.val_f1(torch.sigmoid(y_hat), y.int())\n",
    "        precision = self.val_precision(torch.sigmoid(y_hat), y.int())\n",
    "        recall = self.val_recall(torch.sigmoid(y_hat), y.int())\n",
    "        \n",
    "        self.log('val_loss', val_loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        self.log('val_auroc', auroc, prog_bar=True)\n",
    "        self.log('val_f1', f1, prog_bar=True)\n",
    "        self.log('val_precision', precision, prog_bar=True)\n",
    "        self.log('val_recall', recall, prog_bar=True)\n",
    "        \n",
    "        return val_loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        image, time_series, wavelet, y = batch\n",
    "        y_hat, _ = self(image, time_series, wavelet)\n",
    "        test_loss = self.criterion(y_hat, y)\n",
    "        \n",
    "        # Log metrics\n",
    "        acc = self.val_acc(y_hat, y.int())\n",
    "        auroc = self.val_auroc(y_hat, y.int())\n",
    "        f1 = self.val_f1(y_hat, y.int())\n",
    "        precision = self.val_precision(y_hat, y.int())\n",
    "        recall = self.val_recall(y_hat, y.int())\n",
    "        \n",
    "        self.log('test_loss', test_loss)\n",
    "        self.log('test_acc', acc)\n",
    "        self.log('test_auroc', auroc)\n",
    "        self.log('test_f1', f1)\n",
    "        self.log('test_precision', precision)\n",
    "        self.log('test_recall', recall)\n",
    "        \n",
    "        return test_loss\n",
    "    \n",
    "    def enable_dropout(self):\n",
    "        \"\"\" Enable dropout layers for Monte Carlo Dropout uncertainty estimation \"\"\"\n",
    "        self.dropout_active = True\n",
    "        for m in self.modules():\n",
    "            if m.__class__.__name__.startswith('Dropout'):\n",
    "                m.train()\n",
    "    \n",
    "    def disable_dropout(self):\n",
    "        \"\"\" Disable dropout for normal prediction \"\"\"\n",
    "        self.dropout_active = False\n",
    "        self.eval()\n",
    "    \n",
    "    def predict_with_uncertainty(self, image, time_series, wavelet):\n",
    "        \"\"\" Use Monte Carlo Dropout for uncertainty estimation \"\"\"\n",
    "        self.eval()\n",
    "        self.enable_dropout()\n",
    "        \n",
    "        predictions = []\n",
    "        for _ in range(self.mc_iterations):\n",
    "            with torch.no_grad():\n",
    "                pred, _ = self(image, time_series, wavelet)\n",
    "                predictions.append(pred.unsqueeze(0))\n",
    "        \n",
    "        # Stack predictions\n",
    "        predictions = torch.cat(predictions, dim=0)  # [mc_iterations, batch_size]\n",
    "        \n",
    "        # Calculate mean and variance\n",
    "        mean_pred = torch.mean(predictions, dim=0)\n",
    "        var_pred = torch.var(predictions, dim=0)\n",
    "        \n",
    "        self.disable_dropout()\n",
    "        return mean_pred, var_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Lightning model\n",
    "pl_model = CoralLightningModel(\n",
    "    time_steps=time_steps, \n",
    "    num_features=num_features, \n",
    "    wavelet_dim=X_wavelet.shape[1],\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "# Initialize callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='./checkpoints/',\n",
    "    filename='coral-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=3,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "# Initialize logger\n",
    "logger = TensorBoardLogger('tb_logs', name='coral_model')\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    callbacks=[early_stopping, checkpoint_callback, lr_monitor],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    "    deterministic=True,\n",
    "    precision='16-mixed' if torch.cuda.is_available() else 32  # Mixed precision training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.fit(pl_model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "trainer.test(pl_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Evaluation and Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_attention(model, dataloader):\n",
    "    model.eval()\n",
    "    model = model.to(device)  # Ensure model is on the correct device\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_attention_weights = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image, time_series, wavelet, labels in dataloader:\n",
    "            # Move all tensors to the same device as the model\n",
    "            image = image.to(device)\n",
    "            time_series = time_series.to(device)\n",
    "            wavelet = wavelet.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            preds, attention_weights = model(image, time_series, wavelet)\n",
    "            \n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_attention_weights.append(attention_weights.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(all_preds), np.concatenate(all_labels), all_attention_weights\n",
    "\n",
    "# Initialize the model\n",
    "model = CoralNet(time_steps=time_steps, num_features=num_features, wavelet_dim=X_wavelet.shape[1]).to(device)\n",
    "print(model)\n",
    "\n",
    "# Make predictions\n",
    "y_pred, y_true, attention_weights = predict_with_attention(pl_model.model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and visualize evaluation metrics\n",
    "def plot_evaluation_metrics(y_true, y_pred):\n",
    "    # Convert probabilities to binary predictions\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred_binary)\n",
    "    precision = precision_score(y_true, y_pred_binary)\n",
    "    recall = recall_score(y_true, y_pred_binary)\n",
    "    f1 = f1_score(y_true, y_pred_binary)\n",
    "    auroc = roc_auc_score(y_true, y_pred)\n",
    "    \n",
    "    # Display metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUROC: {auroc:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred_binary, target_names=['Healthy', 'Bleached']))\n",
    "    \n",
    "    # Create figure for visualizations\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred_binary)\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues',\n",
    "        xticklabels=['Healthy', 'Bleached'],\n",
    "        yticklabels=['Healthy', 'Bleached'],\n",
    "        ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title('Confusion Matrix')\n",
    "    axes[0].set_xlabel('Predicted Label')\n",
    "    axes[0].set_ylabel('True Label')\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    axes[1].plot(fpr, tpr, lw=2, label=f'ROC curve (AUC = {auroc:.2f})')\n",
    "    axes[1].plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    axes[1].set_xlim([0.0, 1.0])\n",
    "    axes[1].set_ylim([0.0, 1.05])\n",
    "    axes[1].set_xlabel('False Positive Rate')\n",
    "    axes[1].set_ylabel('True Positive Rate')\n",
    "    axes[1].set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    axes[1].legend(loc='lower right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_pred)\n",
    "    ap = average_precision_score(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(recall_vals, precision_vals, lw=2, label=f'PR curve (AP = {ap:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Distribution of prediction probabilities\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    sns.histplot(y_pred[y_true == 0], bins=20, alpha=0.5, label='Healthy', color='green')\n",
    "    sns.histplot(y_pred[y_true == 1], bins=20, alpha=0.5, label='Bleached', color='red')\n",
    "    \n",
    "    plt.axvline(x=0.5, color='black', linestyle='--', label='Decision Threshold')\n",
    "    plt.xlabel('Prediction Probability')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Prediction Probabilities')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "plot_evaluation_metrics(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncertainty estimation with Monte Carlo Dropout\n",
    "def estimate_uncertainty(model, dataloader, num_samples=5):\n",
    "    # Get a batch of samples\n",
    "    for image, time_series, wavelet, labels in dataloader:\n",
    "        # Select a few samples\n",
    "        image = image[:num_samples].to(device)\n",
    "        time_series = time_series[:num_samples].to(device)\n",
    "        wavelet = wavelet[:num_samples].to(device)\n",
    "        labels = labels[:num_samples].to(device)\n",
    "        break\n",
    "    \n",
    "    # Get predictions with uncertainty\n",
    "    mean_preds, var_preds = model.predict_with_uncertainty(image, time_series, wavelet)\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    mean_preds = mean_preds.cpu().numpy()\n",
    "    var_preds = var_preds.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Sort by prediction mean for better visualization\n",
    "    indices = np.argsort(mean_preds)\n",
    "    mean_preds = mean_preds[indices]\n",
    "    var_preds = var_preds[indices]\n",
    "    labels = labels[indices]\n",
    "    \n",
    "    # Calculate standard deviation and error bars\n",
    "    std_preds = np.sqrt(var_preds)\n",
    "    error_bars = 1.96 * std_preds  # 95% confidence interval\n",
    "    \n",
    "    # Plot each point individually with its own color\n",
    "    for i in range(num_samples):\n",
    "        color = 'green' if labels[i] == 0 else 'red'\n",
    "        plt.errorbar(\n",
    "            i, \n",
    "            mean_preds[i], \n",
    "            yerr=error_bars[i],\n",
    "            fmt='o', \n",
    "            capsize=5, \n",
    "            ecolor='blue',\n",
    "            color=color\n",
    "        )\n",
    "    \n",
    "    plt.axhline(y=0.5, color='black', linestyle='--', label='Decision Threshold')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Prediction Probability')\n",
    "    plt.title('Prediction Uncertainty (95% Confidence Intervals)')\n",
    "    plt.legend(['Decision Threshold', 'Healthy', 'Bleached'])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print details\n",
    "    for i in range(num_samples):\n",
    "        true_label = 'Healthy' if labels[i] == 0 else 'Bleached'\n",
    "        pred_label = 'Healthy' if mean_preds[i] < 0.5 else 'Bleached'\n",
    "        print(f\"Sample {i+1}: True Label = {true_label}, Prediction = {mean_preds[i]:.4f} ± {1.96*std_preds[i]:.4f} → {pred_label}\")\n",
    "\n",
    "\n",
    "# Estimate uncertainty\n",
    "estimate_uncertainty(pl_model, test_loader, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention weights for time series\n",
    "def visualize_attention(time_series_data, attention_matrix, labels, sample_idx):\n",
    "    # Select a sample\n",
    "    ts_data = time_series_data[sample_idx]\n",
    "    attention = attention_matrix[sample_idx]\n",
    "    true_label = 'Bleached' if labels[sample_idx] == 1 else 'Healthy'\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12), gridspec_kw={'height_ratios': [1, 1]})\n",
    "    \n",
    "    # Plot time series data\n",
    "    for f in range(num_features):\n",
    "        ax1.plot(ts_data[:, f], label=feature_names[f])\n",
    "    \n",
    "    ax1.set_title(f\"Time Series Data for Sample {sample_idx} (True Label: {true_label})\")\n",
    "    ax1.set_xlabel(\"Time Steps\")\n",
    "    ax1.set_ylabel(\"Value\")\n",
    "    ax1.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot attention heatmap\n",
    "    im = ax2.imshow(attention, cmap='viridis')\n",
    "    ax2.set_title(\"Attention Weights\")\n",
    "    ax2.set_xlabel(\"Time Steps (Query)\")\n",
    "    ax2.set_ylabel(\"Time Steps (Key)\")\n",
    "    fig.colorbar(im, ax=ax2, label=\"Attention Weight\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find most important time steps\n",
    "    avg_attention = np.mean(attention, axis=0)  # Average attention across all time steps\n",
    "    top_timesteps = np.argsort(avg_attention)[-5:][::-1]  # Top 5 time steps with highest attention\n",
    "    \n",
    "    print(f\"Top 5 most important time steps for sample {sample_idx}: {top_timesteps}\")\n",
    "    \n",
    "    # Visualize features at important time steps\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot feature values at important time steps\n",
    "    bar_width = 0.15\n",
    "    index = np.arange(len(feature_names))\n",
    "    \n",
    "    for i, ts in enumerate(top_timesteps):\n",
    "        plt.bar(index + i*bar_width, ts_data[ts, :], bar_width, label=f'Time Step {ts}')\n",
    "    \n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'Feature Values at Important Time Steps (Sample {sample_idx})')\n",
    "    plt.xticks(index + bar_width * (len(top_timesteps)-1)/2, feature_names, rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get some sample time series data\n",
    "def get_time_series_batch(dataloader, num_samples=5):\n",
    "    batch_ts = []\n",
    "    batch_labels = []\n",
    "    \n",
    "    for _, time_series, _, labels in dataloader:\n",
    "        batch_ts.append(time_series[:num_samples].cpu().numpy())\n",
    "        batch_labels.append(labels[:num_samples].cpu().numpy())\n",
    "        if len(batch_ts) * batch_ts[0].shape[0] >= num_samples:\n",
    "            break\n",
    "    \n",
    "    return np.vstack(batch_ts)[:num_samples], np.hstack(batch_labels)[:num_samples]\n",
    "\n",
    "# Get time series batch\n",
    "ts_batch, ts_labels = get_time_series_batch(test_loader, num_samples=5)\n",
    "\n",
    "# Visualize attention for a couple of samples\n",
    "for i in range(2):  # Visualize attention for 2 samples\n",
    "    visualize_attention(ts_batch, attention_weights[0], ts_labels, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, dataloader):\n",
    "    # Clear GPU memory before starting\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    try:\n",
    "        # Get a batch of data and use only a subset\n",
    "        for image, time_series, wavelet, labels in dataloader:\n",
    "            # Use only 2 samples and keep on CPU\n",
    "            batch_image = image[:2].cpu()\n",
    "            batch_ts = time_series[:2].cpu()\n",
    "            batch_wavelet = wavelet[:2].cpu()\n",
    "            break\n",
    "        \n",
    "        # Create baseline (zero tensor with same shape as input)\n",
    "        baseline = torch.zeros_like(batch_ts)\n",
    "        \n",
    "        # Move model to CPU temporarily for analysis\n",
    "        model = model.cpu()\n",
    "        \n",
    "        # Create a wrapper for the model that only varies the time series input\n",
    "        def model_wrapper(inputs):\n",
    "            return model(batch_image, inputs, batch_wavelet)[0]\n",
    "        \n",
    "        # Initialize Integrated Gradients\n",
    "        ig = IntegratedGradients(model_wrapper)\n",
    "        \n",
    "        print(\"Starting attribution calculation...\")\n",
    "        # Calculate attributions with fewer steps\n",
    "        attributions = ig.attribute(batch_ts, baseline, n_steps=10)\n",
    "        print(\"Attribution calculation complete.\")\n",
    "        \n",
    "        # Average attributions over time steps\n",
    "        feature_importance = torch.abs(attributions).mean(dim=1).numpy()\n",
    "        \n",
    "        # Calculate mean feature importance across the batch\n",
    "        mean_importance = feature_importance.mean(axis=0)\n",
    "        \n",
    "        # Create feature names for time series features\n",
    "        feature_names = [\n",
    "            'Temperature',\n",
    "            'Salinity',\n",
    "            'Dissolved O2',\n",
    "            'pH',\n",
    "            'Turbidity',\n",
    "            'Chlorophyll',\n",
    "            'Depth',\n",
    "            'Light'\n",
    "        ]\n",
    "        \n",
    "        # Sort features by importance\n",
    "        sorted_idx = np.argsort(mean_importance)[::-1]\n",
    "        sorted_features = [feature_names[i] for i in sorted_idx]\n",
    "        sorted_importance = mean_importance[sorted_idx]\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(range(len(sorted_features)), sorted_importance)\n",
    "        plt.yticks(range(len(sorted_features)), sorted_features)\n",
    "        plt.xlabel('Mean Absolute Attribution')\n",
    "        plt.title('Environmental Feature Importance Analysis')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print top features with their importance scores\n",
    "        print(\"\\nFeature Importance Ranking:\")\n",
    "        for i in range(len(sorted_features)):\n",
    "            print(f\"{i+1}. {sorted_features[i]}: {sorted_importance[i]:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during feature importance analysis: {str(e)}\")\n",
    "        print(\"\\nInput tensor shapes:\")\n",
    "        print(f\"Image shape: {batch_image.shape}\")\n",
    "        print(f\"Time series shape: {batch_ts.shape}\")\n",
    "        print(f\"Wavelet shape: {batch_wavelet.shape}\")\n",
    "        \n",
    "    finally:\n",
    "        # Move model back to GPU and clean up\n",
    "        model = model.to(device)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Analyze feature importance\n",
    "analyze_feature_importance(pl_model.model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GradCAM to visualize important regions in images\n",
    "def visualize_gradcam(model, dataloader):\n",
    "    # Get a sample batch\n",
    "    for image, time_series, wavelet, labels in dataloader:\n",
    "        batch_image = image[:5].to(device)  # Just use a few samples\n",
    "        batch_ts = time_series[:5].to(device)\n",
    "        batch_wavelet = wavelet[:5].to(device)\n",
    "        batch_labels = labels[:5].cpu().numpy()\n",
    "        break\n",
    "    \n",
    "    # Create wrapper for model to get Grad-CAM\n",
    "    # Note: This is simplified - for a full implementation you need\n",
    "    # to hook into specific layers of the CNN backbone\n",
    "    \n",
    "    # Get predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds, _ = model(batch_image, batch_ts, batch_wavelet)\n",
    "        preds = preds.cpu().numpy()\n",
    "    \n",
    "    # Convert images to numpy for display\n",
    "    images_np = batch_image.cpu().numpy()\n",
    "    \n",
    "    # Visualize images with predictions\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i in range(min(5, len(images_np))):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        \n",
    "        # Denormalize image\n",
    "        img = images_np[i].transpose(1, 2, 0)\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img = std * img + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        plt.imshow(img)\n",
    "        true_label = 'Bleached' if batch_labels[i] == 1 else 'Healthy'\n",
    "        pred_label = 'Bleached' if preds[i] > 0.5 else 'Healthy'\n",
    "        plt.title(f\"True: {true_label}, Pred: {pred_label} ({preds[i]:.2f})\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Note: For a complete Grad-CAM visualization, specific CNN layers would need to be accessed\")\n",
    "    print(\"This example only shows the original images with their predictions.\")\n",
    "\n",
    "# Visualize with Grad-CAM (simplified)\n",
    "visualize_gradcam(pl_model.model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Deployment and Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model optimization with TorchScript\n",
    "def optimize_model_for_deployment(model, example_inputs):\n",
    "    model.eval()\n",
    "    \n",
    "    # Trace the model with example inputs\n",
    "    traced_model = torch.jit.trace(model, example_inputs)\n",
    "    \n",
    "    # Save the traced model\n",
    "    traced_model.save(\"coral_model_traced.pt\")\n",
    "    \n",
    "    print(\"Model traced and saved as 'coral_model_traced.pt'\")\n",
    "    \n",
    "    # Compare inference speed\n",
    "    def benchmark(model, inputs, n_iterations=100):\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(n_iterations):\n",
    "                _ = model(*inputs)\n",
    "        end_time = time.time()\n",
    "        return (end_time - start_time) / n_iterations\n",
    "    \n",
    "    # Get example inputs\n",
    "    image, time_series, wavelet = example_inputs\n",
    "    \n",
    "    # Benchmark original model\n",
    "    orig_time = benchmark(model, example_inputs)\n",
    "    \n",
    "    # Benchmark traced model\n",
    "    traced_time = benchmark(traced_model, example_inputs)\n",
    "    \n",
    "    print(f\"Original model average inference time: {orig_time*1000:.2f} ms\")\n",
    "    print(f\"Traced model average inference time: {traced_time*1000:.2f} ms\")\n",
    "    print(f\"Speedup: {orig_time/traced_time:.2f}x\")\n",
    "    \n",
    "    return traced_model\n",
    "\n",
    "# Get example inputs for tracing\n",
    "def get_example_inputs(dataloader):\n",
    "    for image, time_series, wavelet, _ in dataloader:\n",
    "        # Just take one example\n",
    "        return (\n",
    "            image[:1].to(device),\n",
    "            time_series[:1].to(device),\n",
    "            wavelet[:1].to(device)\n",
    "        )\n",
    "\n",
    "# Optimize model\n",
    "example_inputs = get_example_inputs(test_loader)\n",
    "traced_model = optimize_model_for_deployment(pl_model.model, example_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple inference function\n",
    "def predict(model, image, time_series, wavelet):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred, _ = model(image, time_series, wavelet)\n",
    "    return pred.item()\n",
    "\n",
    "# Example usage of the inference function\n",
    "def test_inference():\n",
    "    # Get a sample for testing\n",
    "    image, time_series, wavelet, label = next(iter(test_loader))\n",
    "    \n",
    "    # Select a single sample\n",
    "    image = image[0:1].to(device)\n",
    "    time_series = time_series[0:1].to(device)\n",
    "    wavelet = wavelet[0:1].to(device)\n",
    "    true_label = label[0].item()\n",
    "    \n",
    "    # Make prediction\n",
    "    pred = predict(pl_model.model, image, time_series, wavelet)\n",
    "    pred_label = 'Bleached' if pred > 0.5 else 'Healthy'\n",
    "    true_label_text = 'Bleached' if true_label == 1 else 'Healthy'\n",
    "    \n",
    "    print(f\"True label: {true_label_text} ({true_label})\")\n",
    "    print(f\"Predicted label: {pred_label} ({pred:.4f})\")\n",
    "    \n",
    "    # Show the image\n",
    "    img = image[0].cpu().numpy().transpose(1, 2, 0)\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"True: {true_label_text}, Predicted: {pred_label} ({pred:.4f})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show time series data\n",
    "    ts = time_series[0].cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for f in range(num_features):\n",
    "        plt.plot(ts[:, f], label=feature_names[f])\n",
    "    \n",
    "    plt.title(f\"Time Series Data\")\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test the inference\n",
    "test_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion and Next Steps\n",
    "\n",
    "This notebook has implemented a comprehensive, state-of-the-art model for coral reef acidification prediction with significant improvements over the original model:\n",
    "\n",
    "1. **Enhanced Model Architecture**:\n",
    "   - Pre-trained image backbone (EfficientNet)\n",
    "   - Bidirectional LSTM with self-attention for time series\n",
    "   - Advanced feature fusion strategies\n",
    "   - Time-frequency features through wavelet analysis\n",
    "\n",
    "2. **Improved Training Process**:\n",
    "   - Class-balanced sampling\n",
    "   - Mixed precision training\n",
    "   - Advanced learning rate scheduling\n",
    "   - Early stopping and model checkpointing\n",
    "\n",
    "3. **Uncertainty Quantification**:\n",
    "   - Monte Carlo dropout for prediction uncertainty\n",
    "   - Confidence intervals for predictions\n",
    "\n",
    "4. **Advanced Explainability**:\n",
    "   - Feature importance analysis\n",
    "   - Attention visualization\n",
    "   - Integrated gradients for attribution\n",
    "\n",
    "5. **Model Optimization**:\n",
    "   - TorchScript for deployment\n",
    "   - Performance benchmarking\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Transfer Learning with Real Data**:\n",
    "   - Use this architecture with real coral reef data\n",
    "   - Fine-tune the model on domain-specific datasets\n",
    "\n",
    "2. **Deploy to Edge Devices**:\n",
    "   - Optimize for deployment on underwater robots or buoy systems\n",
    "   - Quantization for reduced model size\n",
    "\n",
    "3. **Ensemble Methods**:\n",
    "   - Train multiple models with different architectures\n",
    "   - Create ensemble for improved robustness\n",
    "\n",
    "4. **Incorporate Additional Data Sources**:\n",
    "   - Satellite imagery\n",
    "   - Ocean current data\n",
    "   - Weather patterns\n",
    "\n",
    "5. **Expand to Multiclass Prediction**:\n",
    "   - Predict different stages of coral bleaching\n",
    "   - Identify different types of coral diseases\n",
    "\n",
    "This enhanced model provides a strong foundation for accurate coral reef health monitoring with robust uncertainty estimates and explainable predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
